{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "### Notebook con el scraper para armar el dataset de noticias de Venezuela\n",
    "## Features iniciales\n",
    "### titulo: titulo de la noticia\n",
    "### texto: cuerpo de la noticia\n",
    "### fecha: fecha de publicación de a noticia\n",
    "### link: dirección url de la noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper\n",
    "import datetime \n",
    "import dateutil.parser\n",
    "from datetime import date, timedelta\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#función para convertir fecha de isoformat a datetime\n",
    "def getDateTimeFromISO8601String(iso): \n",
    "    f = dateutil.parser.parse(iso) \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seteo del límite de la cantidad de noticias por sitio web\n",
    "LIMIT = 100\n",
    "\n",
    "data = {}\n",
    "data['newspapers'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el JSON de archivos con las urls a los diarios en línea\n",
    "with open('periodicos.json') as data_file:\n",
    "    companies = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Construyendo el sitio para  universopg1\n",
      "Sitio construido para  universopg1 \n",
      "\n",
      "Van  1  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/16/nota/7191752/juan-guaido-organiza-voluntarios-recibir-ayuda-humanitaria-maduro\n",
      "Van  2  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7190921/oea-propone-reformas-urgentes-profundas-propiciar-elecciones\n",
      "Van  3  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7190501/gobierno-nicolas-maduro-decomiso-donaciones-personas-vih\n",
      "Van  4  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7190258/aviones-militares-estados-unidos-llevaran-venezuela-mas-ayuda\n",
      "Van  5  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7190204/grupo-contacto-sobre-venezuela-enviara-mision-tecnica-proxima\n",
      "Van  6  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7190195/presidente-colombia-pide-oea-mas-presion-sobre-nicolas-maduro\n",
      "Van  7  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7189514/huir-o-comida-migran-mas-venezolanos\n",
      "Van  8  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/15/nota/7189514/huir-o-comida-migran-mas-venezolanos\n",
      "Van  9  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/14/nota/7189502/nicolas-maduro-revela-reuniones-enviado-donald-trump-venezuela\n",
      "Van  10  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/14/nota/7189502/nicolas-maduro-revela-reuniones-enviado-donald-trump-venezuela\n",
      "Van  11  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/14/nota/7189388/multimillonario-richard-branson-organiza-concierto-recaudar-dinero\n",
      "Van  12  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/noticias/2019/02/14/nota/7188599/diputado-opositor-dice-que-se-incluira-militares-chavismo-maduro\n",
      "Van  13  artículos bajados de  universopg1  usando el sitio de url:  https://www.eluniverso.com/2019/02/14/video/7189172/gobierno-venezuela-anuncia-apoyo-50-paises-onu\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "\n",
    "# Iteración por cada diario electrónico\n",
    "for company, value in companies.items():\n",
    "    print(\"\\nConstruyendo el sitio para \", company)\n",
    "    paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "    newsPaper = {\n",
    "        \"link\": value['link'],\n",
    "        \"articles\": []\n",
    "    }\n",
    "    print(\"Sitio construido para \", company,\"\\n\")\n",
    "    noneTypeCount = 0\n",
    "    for content in paper.articles:\n",
    "        if count > LIMIT:\n",
    "            break\n",
    "        try:\n",
    "            content.download()\n",
    "            content.parse()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"continuando...\\n\")\n",
    "            continue\n",
    "        # Se saltará el artículo que no posea fecha de publicación.\n",
    "        # Después de tener 10 artículos sin fecha, el diario electrónico será saltado, seguramente su estructura no facilita la obtención de los features seleccionados.\n",
    "        if content.publish_date is None or content.title is None or content.text is None or content.url is None:\n",
    "            print(\"El artícuo tiene tipo None...\")\n",
    "            noneTypeCount = noneTypeCount + 1\n",
    "            if noneTypeCount > 10:\n",
    "                print(\"Demasiado tipo None, abortando...\\n\")\n",
    "                noneTypeCount = 0\n",
    "                break\n",
    "            continue\n",
    "        else:\n",
    "            #print(content.title)\n",
    "            #print(content.text)\n",
    "            #print(content.url)\n",
    "            #print(content.publish_date.isoformat())\n",
    "            #fecha = getDateTimeFromISO8601String(content.publish_date.isoformat())\n",
    "            #fecha_desde = datetime(2018, 1, 1, 0, 0, 0)\n",
    "            #fecha_hasta = datetime(2018,12,31,23,59,59)\n",
    "            palabras = [\"Maduro\",\"Venezuela\",\"venezolanos\",\"Venezuelan\",\"venezolanas\",\"venezolano\"]\n",
    "            for palabra in palabras:\n",
    "                if palabra in content.title:\n",
    "                    #if fecha <= fecha_hasta and fecha >= fecha_desde:\n",
    "                    article = {}\n",
    "                    article['titulo'] = content.title\n",
    "                    article['texto'] = content.text\n",
    "                    article['link'] = content.url\n",
    "                    article['fecha'] = content.publish_date.isoformat()\n",
    "                    newsPaper['articles'].append(article)\n",
    "                    print(\"Van \",count, \" artículos bajados de \", company, \" usando el sitio de url: \", content.url)\n",
    "                    count = count + 1\n",
    "                    noneTypeCount = 0\n",
    "                    data['newspapers'][company] = newsPaper\n",
    "                    try:\n",
    "                        with open('articulos_scrapeados2.json', 'w') as outfile:\n",
    "                            json.dump(data, outfile)\n",
    "                    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {}\n",
    "data['nombre'] = 'Jose'\n",
    "data['edad'] = '15'\n",
    "data['nacionalidad'] = 'Mex'\n",
    "\n",
    "dir = 'C:/Users/User/Thalía/Espol/2018-2S/Minería/noticiasVenezuela'\n",
    "file_name = \"data.json\"\n",
    "\n",
    "with open(os.path.join(dir, file_name), 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
